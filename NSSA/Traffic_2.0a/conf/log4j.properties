# Define the root logger with appender file
#生产环境
#log = /home/workspace/DataminingServer/logs
#测试
#log =/home/hadoop/duan/DataminingServer/logs
#本机
log =../logs
#定义LOG输出级别
#log4j.rootLogger = ERROR, FILE
log4j.rootLogger =WARN,STDOUT, FILE
spark.root.logger=${log4j.rootLogger}


#add the bestparam log
log4j.logger.org.apache.spark.ml.tuning.TrainValidationSplit=INFO
log4j.logger.org.apache.spark.ml.tuning.CrossValidator=INFO

# 定义文件file appender
log4j.appender.FILE=org.apache.log4j.RollingFileAppender
#设置日志文件最大大小
log4j.appender.FILE.MaxFileSize=10MB
log4j.appender.FILE.MaxBackupIndex=3
log4j.appender.FILE.File=${log}/datamining.log
#log4j.appender.FILE.File=${log}/testLog.log
# 定义控制台 STDOUT appender
log4j.appender.STDOUT=org.apache.log4j.ConsoleAppender
log4j.appender.STDOUT.Target=System.out

# 定义日志输出目的地为文件
log4j.appender.FILE.layout=org.apache.log4j.PatternLayout
log4j.appender.FILE.layout.conversionPattern=%m%n
#定义日志输出目的地为控制台
log4j.appender.STDOUT.layout=org.apache.log4j.PatternLayout
log4j.appender.STDOUT.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss}%m%n


